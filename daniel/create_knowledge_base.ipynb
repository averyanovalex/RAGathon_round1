{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-04T08:17:12.377668Z",
     "start_time": "2024-09-04T08:17:12.363156Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T08:17:12.443356Z",
     "start_time": "2024-09-04T08:17:12.413640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import PyPDF2\n",
    "\n",
    "def split_rec(full_text):\n",
    "    # Initialize tiktoken encoding for the specified model\n",
    "    encoding = tiktoken.encoding_for_model(OPENAI_MODEL)    \n",
    "    # Count the number of tokens in the extracted text\n",
    "    num_tokens = len(encoding.encode(full_text))\n",
    "    \n",
    "    # Check if the number of tokens exceeds the limit (128K)\n",
    "    # or the number of characters \n",
    "    if num_tokens > 100000 or len(full_text) > 900000:\n",
    "        print(\"splitting\")\n",
    "        # Split the pages into two roughly equal parts\n",
    "        mid_point = len(full_text) // 2\n",
    "        part1 = full_text[:mid_point]\n",
    "        part2 = full_text[mid_point:]\n",
    "        \n",
    "        split1 = split_rec(part1)\n",
    "        split2 = split_rec(part2)\n",
    "        \n",
    "        return split1 + split2\n",
    "    else:\n",
    "        # Return the full text as a single-element list\n",
    "        return [full_text]    \n",
    "\n",
    "def text_from_pdf(file_path):\n",
    "    # Initialize an empty list to store the text of each page\n",
    "    pages_text = []\n",
    "\n",
    "    # Read the PDF file\n",
    "    with open(file_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f, strict=False)\n",
    "        for page in reader.pages:\n",
    "            pages_text.append(page.extract_text())\n",
    "    \n",
    "    # Combine all the text to check the total token count\n",
    "    full_text = \"\\n\".join(pages_text)\n",
    "\n",
    "    return split_rec(full_text)"
   ],
   "id": "73aa1ff879683fbb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T08:17:12.549774Z",
     "start_time": "2024-09-04T08:17:12.547370Z"
    }
   },
   "cell_type": "code",
   "source": "OPENAI_MODEL=\"gpt-4o-2024-08-06\"",
   "id": "da8cea1c74a874e0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T08:17:13.119228Z",
     "start_time": "2024-09-04T08:17:12.599381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "def merge_usage_report_items(items):\n",
    "    return {\n",
    "        \"my_tokens\": sum([i[\"my_tokens\"] for i in items]),\n",
    "        \"prompt_tokens\": sum([i[\"prompt_tokens\"] for i in items]),\n",
    "        \"completion_tokens\": sum([i[\"completion_tokens\"] for i in items]),\n",
    "        \"total_tokens\": sum([i[\"total_tokens\"] for i in items]),\n",
    "        \"total_cost_usd\": sum([i[\"total_cost_usd\"] for i in items]),\n",
    "    }\n",
    "\n",
    "enc = tiktoken.encoding_for_model(OPENAI_MODEL)\n",
    "input_token_cost_usd_per_1m_tokens = 2.5\n",
    "output_token_cost_usd_per_1m_tokens = 10\n",
    "_1m = 1000000\n",
    "\n",
    "def get_usage_report(messages, response):\n",
    "    content = \" \".join([m[\"content\"] for m in messages])\n",
    "    my_tokens = len(enc.encode(content))\n",
    "    print(f\"My Tokens: {my_tokens}\")\n",
    "\n",
    "    prompt_tokens = response.usage.prompt_tokens\n",
    "    print(f\"Prompt Tokens: {prompt_tokens}\")\n",
    "\n",
    "    completion_tokens = response.usage.completion_tokens\n",
    "    print(f\"Completion Tokens: {completion_tokens}\")\n",
    "\n",
    "    prompt_cost_in_usd = (prompt_tokens / _1m) * input_token_cost_usd_per_1m_tokens\n",
    "    completion_cost_in_usd = (completion_tokens / _1m) * output_token_cost_usd_per_1m_tokens\n",
    "    total_cost_usd = prompt_cost_in_usd + completion_cost_in_usd\n",
    "    print(f\"Cost: ${total_cost_usd}\")\n",
    "    \n",
    "    total_tokens = response.usage.total_tokens\n",
    "    \n",
    "    if total_tokens != prompt_tokens + completion_tokens:\n",
    "        print(\"WARN: token counts don't match\")\n",
    "        print(total_tokens)\n",
    "        print(prompt_tokens)\n",
    "        print(completion_tokens)\n",
    "        print(prompt_tokens+completion_tokens)\n",
    "    \n",
    "    return {\n",
    "        \"my_tokens\": my_tokens,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"completion_tokens\": completion_tokens,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"total_cost_usd\": total_cost_usd,        \n",
    "    }"
   ],
   "id": "a7a2ffc328682dd5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T08:17:13.213024Z",
     "start_time": "2024-09-04T08:17:13.126093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from enum import Enum\n",
    "\n",
    "class Metric(str, Enum):\n",
    "    rad_expenses = \"research and development expenses\"\n",
    "    risk_management_spending = \"risk management spending\"\n",
    "#    debt_to_equity_ratio = \"Debt-To-Equity ratio\"\n",
    "#    number_of_stores = \"Number of stores\"\n",
    "    return_on_assets = \"Return on Assets (ROA)\"\n",
    "#    return_on_equity = \"Return on Equity (ROE)\"\n",
    "    customer_acquisition_spending = \"customer acquisition spending\"\n",
    "    operating_margin = \"operating margin\"\n",
    "    market_capitalization = \"market capitalization\"\n",
    "    sustainability_initiatives_spending = \"sustainability initiatives spending\"\n",
    "    gross_profit_margin = \"gross profit margin\"\n",
    "    net_profit_margin = \"net profit margin\"\n",
    "    total_liabilities = \"total liabilities\"\n",
    "    total_assets = \"total assets\"\n",
    "    intangible_assets = \"intangible assets\"\n",
    "    marketing_spending = \"marketing spending\"\n",
    "    free_cash_flow = \"free cash flow\"\n",
    "    earnings_per_share = \"earnings per share (EPS)\"\n",
    "    accounts_receivable = \"accounts_receivable\"\n",
    "    acquisition_costs = \"acquisition costs\"\n",
    "    shareholders_equity = \"shareholders' equity\"\n",
    "    operating_cash_flow = \"operating cash flow\"\n",
    "    quick_ratio = \"Quick Ratio\"\n",
    "    net_income = \"net income\"\n",
    "    inventory = \"inventory\"\n",
    "    total_revenue = \"total revenue\"\n",
    "\n",
    "#class CompanyRole(str, Enum):\n",
    "#    ceo = \"Chief Executive Officer (CEO)\"\n",
    "#    cfo = \"Chief Financial Officer (CFO)\"\n",
    "#    coo = \"Chief Operating Officer (COO)\"\n",
    "#    clo = \"Chief Legal Officer (CLO)\"\n",
    "#    board_chairman = \"Board Chairman\"\n",
    "\n",
    "class Currency(str, Enum):\n",
    "    euro = \"EUR\"\n",
    "    us_dollar = \"USD\"\n",
    "    great_britain_pound = \"GBP\"\n",
    "    australian_dollar = \"AUD\"\n",
    "    other = \"OTHER\"\n",
    "\n",
    "class DocumentDataPoint(BaseModel):\n",
    "    metric_type: Metric\n",
    "    value: float\n",
    "    currency: Optional[Currency]\n",
    "    point_in_time_as_iso_date: str\n",
    "\n",
    "#class CompanyRoleAssignment(BaseModel):\n",
    "#    role_type: CompanyRole\n",
    "#    person_name: str\n",
    "#    role_assignment_started_as_iso_date: Optional[str]\n",
    "#    role_assignment_ended_as_iso_date: Optional[str]\n",
    "\n",
    "class DocumentContent(BaseModel):\n",
    "    data_points: list[DocumentDataPoint]\n",
    "#    company_role_assignments: list[CompanyRoleAssignment]"
   ],
   "id": "3754c275b4be0d9b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T08:17:13.224480Z",
     "start_time": "2024-09-04T08:17:13.219687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_document_content(text):\n",
    "    system_prompt = (\"You are an assistant with the task of extracting precise information from long documents. \"\n",
    "                     \"You will be prompted with the contents of a document. Your task is to extract various metrics \"\n",
    "#                     \"as well as company role assignments \"\n",
    "                     \"from this document. With each metric, supply the point in \"\n",
    "                     \"time when the metric was measured according to the document,\"\n",
    "                     \"as well as the currency (if applicable). \"\n",
    "                     \"If the metric is an amount, extract the exact amount (e.g. \"\n",
    "                     \"if the amount in the document is given as '100 (in thousands)' \"\n",
    "                     \"or '100k', extract the value '100000').\"\n",
    "#                     \"With each role assignment, supply when the role assignment started and ended, if possible.\"\n",
    "                     \"\\n\\n\"\n",
    "                     \"Do your best to include as many metrics for as many points in time as possible!\")                     \n",
    "    \n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "      ]\n",
    "    \n",
    "    \n",
    "    response = client.beta.chat.completions.parse(\n",
    "      model=OPENAI_MODEL,\n",
    "      messages=messages,\n",
    "      response_format=DocumentContent\n",
    "    )\n",
    "    \n",
    "    usage_report = get_usage_report(messages, response)\n",
    "    \n",
    "    data_points = [\n",
    "        {\n",
    "            \"metric_type\": x.metric_type.value,\n",
    "            \"value\": x.value,\n",
    "            \"currency\": x.currency.value if x.currency else None,\n",
    "            \"point_in_time\": x.point_in_time_as_iso_date\n",
    "        }\n",
    "        for x in response.choices[0].message.parsed.data_points\n",
    "    ]\n",
    "    \n",
    "#    role_assignments = [\n",
    "#        {\n",
    "#            \"role_type\": x.role_type.value,\n",
    "#            \"person_name\": x.person_name,\n",
    "#            \"role_assignment_started_as_iso_date\": x.role_assignment_started_as_iso_date,\n",
    "#            \"role_assignment_ended_as_iso_date\": x.role_assignment_ended_as_iso_date\n",
    "#        }\n",
    "#        for x in response.choices[0].message.parsed.company_role_assignments\n",
    "#    ]\n",
    "    \n",
    "    result = {\n",
    "        \"data_points\": data_points,\n",
    "#        \"role_assignments\": role_assignments\n",
    "    }\n",
    "    return result, usage_report"
   ],
   "id": "6a634b01c9d7840a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T08:27:17.744256Z",
     "start_time": "2024-09-04T08:17:13.264016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "\n",
    "\n",
    "# Define the paths\n",
    "samples_dir = 'samples'\n",
    "output_dir = 'output'\n",
    "\n",
    "usage_reports = {}\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read the CSV file\n",
    "with open('dataset.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "    for row in csv_reader:\n",
    "        name = row['sha1'].strip().replace(',', '').replace('\"', '')  # Clean up the name to be used in filenames\n",
    "        pdf_path = os.path.join(samples_dir, f'{name}.pdf')\n",
    "        \n",
    "        # Check if the PDF file exists\n",
    "        if os.path.exists(pdf_path):\n",
    "            # Define the output path for the JSON file\n",
    "            output_path = os.path.join(output_dir, f'{name}.json')\n",
    "\n",
    "            if os.path.exists(output_path):\n",
    "                print(f'{output_path} already exists; skipping.')\n",
    "            else:\n",
    "                print(f'Processing {pdf_path}...')\n",
    "    \n",
    "                try:    \n",
    "                    # Extract text from the PDF\n",
    "                    pdf_texts = text_from_pdf(pdf_path)\n",
    "        \n",
    "                    # Extract structured content from the text\n",
    "                    # Involves \"unzipping\" following https://stackoverflow.com/questions/12974474/how-to-unzip-a-list-of-tuples-into-individual-lists.\n",
    "                    [structured_datas, usage_report_items] = [list(t) for t in zip(*[extract_document_content(pdf_text) for pdf_text in pdf_texts])]\n",
    "                    \n",
    "                    usage_reports[name] = merge_usage_report_items(usage_report_items)\n",
    "                    \n",
    "                    structured_data = {\n",
    "                        \"company_name\": row['name'],\n",
    "                        \"data_points\": [item for d in structured_datas for item in d[\"data_points\"]],\n",
    "#                        \"role_assignments\": [item for d in structured_datas for item in d[\"role_assignments\"]]\n",
    "                    }\n",
    "\n",
    "                    # Save the structured data as JSON\n",
    "                    with open(output_path, 'w') as json_file:\n",
    "                        json.dump(structured_data, json_file, indent=4)\n",
    "        \n",
    "                    print(f'Saved structured data to {output_path}.')\n",
    "                except Exception as error:\n",
    "                    print(error)\n",
    "                    print(\"Exception caught; skipping...\")\n",
    "                # Uncomment this to only work with the first PDF.\n",
    "                # break\n",
    "        else:\n",
    "            # The file was not found. We ignore this, since we are only working\n",
    "            # with a small sample.\n",
    "            pass\n",
    "\n",
    "usage_report = {\n",
    "    \"summary\": merge_usage_report_items(usage_reports.values()),\n",
    "    \"details\": usage_reports\n",
    "}\n",
    "\n",
    "with open('usage_report_create_knowledge_base.json', 'w') as json_file:\n",
    "    json.dump(usage_report, json_file, indent=4)"
   ],
   "id": "36c534d070c07b64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing samples/ac9aa244462c80705c3ff046542c02c459989742.pdf...\n",
      "My Tokens: 79230\n",
      "Prompt Tokens: 79512\n",
      "Completion Tokens: 628\n",
      "Cost: $0.20506000000000002\n",
      "Saved structured data to output/ac9aa244462c80705c3ff046542c02c459989742.json.\n",
      "Processing samples/e2b19d2cc2ccab2fd9022326b56b38fb0e772e73.pdf...\n",
      "My Tokens: 98287\n",
      "Prompt Tokens: 98568\n",
      "Completion Tokens: 1252\n",
      "Cost: $0.25894\n",
      "Saved structured data to output/e2b19d2cc2ccab2fd9022326b56b38fb0e772e73.json.\n",
      "Processing samples/e62b2ebe3012cd7e6c57507bc950a46d06b3d06e.pdf...\n",
      "My Tokens: 51326\n",
      "Prompt Tokens: 51608\n",
      "Completion Tokens: 529\n",
      "Cost: $0.13430999999999998\n",
      "Saved structured data to output/e62b2ebe3012cd7e6c57507bc950a46d06b3d06e.json.\n",
      "Processing samples/e765cdd472cb47fa74ee6a52700c61aca645bbee.pdf...\n",
      "splitting\n",
      "My Tokens: 48532\n",
      "Prompt Tokens: 48814\n",
      "Completion Tokens: 673\n",
      "Cost: $0.12876500000000002\n",
      "My Tokens: 52216\n",
      "Prompt Tokens: 52498\n",
      "Completion Tokens: 891\n",
      "Cost: $0.140155\n",
      "Saved structured data to output/e765cdd472cb47fa74ee6a52700c61aca645bbee.json.\n",
      "Processing samples/6054ec55767fbe6585598ced7afacf5cb8619a13.pdf...\n",
      "My Tokens: 70866\n",
      "Prompt Tokens: 71148\n",
      "Completion Tokens: 659\n",
      "Cost: $0.18446\n",
      "Saved structured data to output/6054ec55767fbe6585598ced7afacf5cb8619a13.json.\n",
      "Processing samples/9d7a72445aba6860402c3acce75af02dc045f74d.pdf...\n",
      "My Tokens: 71338\n",
      "Prompt Tokens: 71620\n",
      "Completion Tokens: 904\n",
      "Cost: $0.18809\n",
      "Saved structured data to output/9d7a72445aba6860402c3acce75af02dc045f74d.json.\n",
      "Processing samples/a706b44ba275c97b8633b0808cd2f90cbb7fe473.pdf...\n",
      "splitting\n",
      "My Tokens: 61877\n",
      "Prompt Tokens: 62159\n",
      "Completion Tokens: 452\n",
      "Cost: $0.1599175\n",
      "My Tokens: 63683\n",
      "Prompt Tokens: 63964\n",
      "Completion Tokens: 588\n",
      "Cost: $0.16579\n",
      "Saved structured data to output/a706b44ba275c97b8633b0808cd2f90cbb7fe473.json.\n",
      "Processing samples/a8077fe1983a64dc77bddfafbf48242e66111a89.pdf...\n",
      "My Tokens: 76059\n",
      "Prompt Tokens: 76341\n",
      "Completion Tokens: 256\n",
      "Cost: $0.19341250000000001\n",
      "Saved structured data to output/a8077fe1983a64dc77bddfafbf48242e66111a89.json.\n",
      "Processing samples/84749ef5c2bbf2a302b6614f31727a95bf29f309.pdf...\n",
      "My Tokens: 46119\n",
      "Prompt Tokens: 46402\n",
      "Completion Tokens: 284\n",
      "Cost: $0.11884499999999999\n",
      "Saved structured data to output/84749ef5c2bbf2a302b6614f31727a95bf29f309.json.\n",
      "Processing samples/ba5852cb6c20da35da2ce6ebaafc711d06fe8c1e.pdf...\n",
      "splitting\n",
      "splitting\n",
      "splitting\n",
      "splitting\n",
      "splitting\n",
      "splitting\n",
      "splitting\n",
      "My Tokens: 64383\n",
      "Prompt Tokens: 64665\n",
      "Completion Tokens: 861\n",
      "Cost: $0.1702725\n",
      "My Tokens: 56968\n",
      "Prompt Tokens: 57250\n",
      "Completion Tokens: 460\n",
      "Cost: $0.147725\n",
      "My Tokens: 60852\n",
      "Prompt Tokens: 61133\n",
      "Completion Tokens: 662\n",
      "Cost: $0.1594525\n",
      "My Tokens: 64595\n",
      "Prompt Tokens: 64877\n",
      "Completion Tokens: 532\n",
      "Cost: $0.1675125\n",
      "My Tokens: 57829\n",
      "Prompt Tokens: 58110\n",
      "Completion Tokens: 452\n",
      "Cost: $0.149795\n",
      "My Tokens: 63274\n",
      "Prompt Tokens: 63556\n",
      "Completion Tokens: 343\n",
      "Cost: $0.16232\n",
      "My Tokens: 60481\n",
      "Prompt Tokens: 60763\n",
      "Completion Tokens: 450\n",
      "Cost: $0.1564075\n",
      "My Tokens: 62501\n",
      "Prompt Tokens: 62783\n",
      "Completion Tokens: 715\n",
      "Cost: $0.16410750000000002\n",
      "Saved structured data to output/ba5852cb6c20da35da2ce6ebaafc711d06fe8c1e.json.\n",
      "Processing samples/609042c64a759c0ac63e7cf18742be4dd3cc5cd5.pdf...\n",
      "splitting\n",
      "My Tokens: 92850\n",
      "Prompt Tokens: 93132\n",
      "Completion Tokens: 280\n",
      "Cost: $0.23563\n",
      "My Tokens: 99322\n",
      "Prompt Tokens: 99604\n",
      "Completion Tokens: 428\n",
      "Cost: $0.25329\n",
      "Saved structured data to output/609042c64a759c0ac63e7cf18742be4dd3cc5cd5.json.\n",
      "Processing samples/e0d6bb578bfc233c3bdbded229960fed01624631.pdf...\n",
      "splitting\n",
      "My Tokens: 81343\n",
      "Prompt Tokens: 81625\n",
      "Completion Tokens: 520\n",
      "Cost: $0.20926250000000002\n",
      "My Tokens: 87989\n",
      "Prompt Tokens: 88272\n",
      "Completion Tokens: 465\n",
      "Cost: $0.22533\n",
      "Saved structured data to output/e0d6bb578bfc233c3bdbded229960fed01624631.json.\n",
      "Processing samples/43437bccca01aeae222ec7cc706ba3eaf50be2d7.pdf...\n",
      "My Tokens: 50368\n",
      "Prompt Tokens: 50650\n",
      "Completion Tokens: 774\n",
      "Cost: $0.13436499999999998\n",
      "Saved structured data to output/43437bccca01aeae222ec7cc706ba3eaf50be2d7.json.\n",
      "Processing samples/194000c9109c6fa628f1fed33b44ae4c2b8365f4.pdf...\n",
      "My Tokens: 98223\n",
      "Prompt Tokens: 98505\n",
      "Completion Tokens: 758\n",
      "Cost: $0.25384249999999997\n",
      "Saved structured data to output/194000c9109c6fa628f1fed33b44ae4c2b8365f4.json.\n",
      "Processing samples/f06d7ecc8072de616a4ea35c74e20199de6b0691.pdf...\n",
      "splitting\n",
      "My Tokens: 48380\n",
      "Prompt Tokens: 48662\n",
      "Completion Tokens: 247\n",
      "Cost: $0.12412499999999999\n",
      "My Tokens: 56641\n",
      "Prompt Tokens: 56923\n",
      "Completion Tokens: 591\n",
      "Cost: $0.1482175\n",
      "Saved structured data to output/f06d7ecc8072de616a4ea35c74e20199de6b0691.json.\n",
      "Processing samples/85fb23ba2910de45e27f8f40170c0f3576043916.pdf...\n",
      "splitting\n",
      "My Tokens: 57591\n",
      "Prompt Tokens: 57873\n",
      "Completion Tokens: 107\n",
      "Cost: $0.14575249999999998\n",
      "My Tokens: 61906\n",
      "Prompt Tokens: 62187\n",
      "Completion Tokens: 1051\n",
      "Cost: $0.1659775\n",
      "Saved structured data to output/85fb23ba2910de45e27f8f40170c0f3576043916.json.\n",
      "Processing samples/2779336b845a41544348abb7b3e6e5bd2ff893a2.pdf...\n",
      "My Tokens: 75157\n",
      "Prompt Tokens: 75439\n",
      "Completion Tokens: 444\n",
      "Cost: $0.19303750000000003\n",
      "Saved structured data to output/2779336b845a41544348abb7b3e6e5bd2ff893a2.json.\n",
      "Processing samples/f721fa86aa0f17b194fa21917aeabbf8df6511c2.pdf...\n",
      "My Tokens: 47670\n",
      "Prompt Tokens: 47951\n",
      "Completion Tokens: 563\n",
      "Cost: $0.1255075\n",
      "Saved structured data to output/f721fa86aa0f17b194fa21917aeabbf8df6511c2.json.\n",
      "Processing samples/cbd8fb252e8743dc53961adc53e2b4de9ff6037a.pdf...\n",
      "splitting\n",
      "My Tokens: 50244\n",
      "Prompt Tokens: 50526\n",
      "Completion Tokens: 472\n",
      "Cost: $0.131035\n",
      "My Tokens: 59924\n",
      "Prompt Tokens: 60206\n",
      "Completion Tokens: 501\n",
      "Cost: $0.15552500000000002\n",
      "Saved structured data to output/cbd8fb252e8743dc53961adc53e2b4de9ff6037a.json.\n",
      "Processing samples/e33544bdea57faa0ad10ba2e93bf052482f33325.pdf...\n",
      "splitting\n",
      "My Tokens: 53604\n",
      "Prompt Tokens: 53885\n",
      "Completion Tokens: 605\n",
      "Cost: $0.1407625\n",
      "My Tokens: 60526\n",
      "Prompt Tokens: 60808\n",
      "Completion Tokens: 1048\n",
      "Cost: $0.16249999999999998\n",
      "Saved structured data to output/e33544bdea57faa0ad10ba2e93bf052482f33325.json.\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
