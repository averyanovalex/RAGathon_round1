{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-03T07:20:50.681594Z",
     "start_time": "2024-09-03T07:20:50.676525Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T07:20:50.696262Z",
     "start_time": "2024-09-03T07:20:50.691614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import PyPDF2\n",
    "\n",
    "def split_rec(full_text):\n",
    "    # Initialize tiktoken encoding for the specified model\n",
    "    encoding = tiktoken.encoding_for_model(OPENAI_MODEL)    \n",
    "    # Count the number of tokens in the extracted text\n",
    "    num_tokens = len(encoding.encode(full_text))\n",
    "    \n",
    "    # Check if the number of tokens exceeds the limit (128K)\n",
    "    # or the number of characters \n",
    "    if num_tokens > 100000 or len(full_text) > 900000:\n",
    "        print(\"splitting\")\n",
    "        # Split the pages into two roughly equal parts\n",
    "        mid_point = len(full_text) // 2\n",
    "        part1 = full_text[:mid_point]\n",
    "        part2 = full_text[mid_point:]\n",
    "        \n",
    "        split1 = split_rec(part1)\n",
    "        split2 = split_rec(part2)\n",
    "        \n",
    "        return split1 + split2\n",
    "    else:\n",
    "        # Return the full text as a single-element list\n",
    "        return [full_text]    \n",
    "\n",
    "def text_from_pdf(file_path):\n",
    "    # Initialize an empty list to store the text of each page\n",
    "    pages_text = []\n",
    "\n",
    "    # Read the PDF file\n",
    "    with open(file_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f, strict=False)\n",
    "        for page in reader.pages:\n",
    "            pages_text.append(page.extract_text())\n",
    "    \n",
    "    # Combine all the text to check the total token count\n",
    "    full_text = \"\\n\".join(pages_text)\n",
    "\n",
    "    return split_rec(full_text)"
   ],
   "id": "73aa1ff879683fbb",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T07:20:50.750203Z",
     "start_time": "2024-09-03T07:20:50.746243Z"
    }
   },
   "cell_type": "code",
   "source": "OPENAI_MODEL=\"gpt-4o-2024-08-06\"",
   "id": "da8cea1c74a874e0",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T07:20:50.798495Z",
     "start_time": "2024-09-03T07:20:50.794299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(OPENAI_MODEL)\n",
    "def prompt_info(messages):\n",
    "    content = \" \".join([m[\"content\"] for m in messages])\n",
    "    tokens = len(enc.encode(content))\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    cost_in_eur = (tokens / 1000000)*5 \n",
    "    print(f\"Cost: {cost_in_eur}â‚¬\")"
   ],
   "id": "a7a2ffc328682dd5",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T07:20:50.845322Z",
     "start_time": "2024-09-03T07:20:50.842295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# FIRST ATTEMPT\n",
    "\n",
    "# system_prompt = (\"You are an assistant with the task of extracting precise information from long documents. \"\n",
    "#                  \"You will be prompted with a USER QUESTION and a DOCUMENT. Your task is to answer the USER QUESTION \"\n",
    "#                  \"precisely and concisely, using only information from the DOCUMENT. If the document does not contain \"\n",
    "#                  \"enough information to answer the question, answer only with the text 'N/A'.\")\n",
    "# \n",
    "# \n",
    "# question = \"What was the total revenue in 2022?\"\n",
    "# \n",
    "# prompt = (\"USER QUESTION\\n\\n\"\n",
    "#           f\"{question}\\n\\n\"\n",
    "#           \"DOCUMENT\\n\\n\"\n",
    "#           f\"{text}\")\n",
    "# \n",
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "# \n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": system_prompt},\n",
    "#     {\"role\": \"user\", \"content\": prompt},\n",
    "#   ]\n",
    "# \n",
    "# prompt_info(messages)"
   ],
   "id": "e4d92c8b9fa4ec84",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T07:20:50.902089Z",
     "start_time": "2024-09-03T07:20:50.890909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from enum import Enum\n",
    "\n",
    "class Metric(str, Enum):\n",
    "    rad_expenses = \"research and development expenses\"\n",
    "    risk_management_spending = \"risk management spending\"\n",
    "#    debt_to_equity_ratio = \"Debt-To-Equity ratio\"\n",
    "#    number_of_stores = \"Number of stores\"\n",
    "    return_on_assets = \"Return on Assets (ROA)\"\n",
    "#    return_on_equity = \"Return on Equity (ROE)\"\n",
    "    customer_acquisition_spending = \"customer acquisition spending\"\n",
    "    operating_margin = \"operating margin\"\n",
    "    market_capitalization = \"market capitalization\"\n",
    "    sustainability_initiatives_spending = \"sustainability initiatives spending\"\n",
    "    gross_profit_margin = \"gross profit margin\"\n",
    "    net_profit_margin = \"net profit margin\"\n",
    "    total_liabilities = \"total liabilities\"\n",
    "    total_assets = \"total assets\"\n",
    "    intangible_assets = \"intangible assets\"\n",
    "    marketing_spending = \"marketing spending\"\n",
    "    free_cash_flow = \"free cash flow\"\n",
    "    earnings_per_share = \"earnings per share (EPS)\"\n",
    "    accounts_receivable = \"accounts_receivable\"\n",
    "    acquisition_costs = \"acquisition costs\"\n",
    "    shareholders_equity = \"shareholders' equity\"\n",
    "    operating_cash_flow = \"operating cash flow\"\n",
    "    quick_ratio = \"Quick Ratio\"\n",
    "    net_income = \"net income\"\n",
    "    inventory = \"inventory\"\n",
    "    total_revenue = \"total revenue\"\n",
    "\n",
    "#class CompanyRole(str, Enum):\n",
    "#    ceo = \"Chief Executive Officer (CEO)\"\n",
    "#    cfo = \"Chief Financial Officer (CFO)\"\n",
    "#    coo = \"Chief Operating Officer (COO)\"\n",
    "#    clo = \"Chief Legal Officer (CLO)\"\n",
    "#    board_chairman = \"Board Chairman\"\n",
    "\n",
    "class Currency(str, Enum):\n",
    "    euro = \"EUR\"\n",
    "    us_dollar = \"USD\"\n",
    "    great_britain_pound = \"GBP\"\n",
    "    australian_dollar = \"AUD\"\n",
    "    other = \"OTHER\"\n",
    "\n",
    "class DocumentDataPoint(BaseModel):\n",
    "    metric_type: Metric\n",
    "    value: float\n",
    "    currency: Optional[Currency]\n",
    "    point_in_time_as_iso_date: str\n",
    "\n",
    "#class CompanyRoleAssignment(BaseModel):\n",
    "#    role_type: CompanyRole\n",
    "#    person_name: str\n",
    "#    role_assignment_started_as_iso_date: Optional[str]\n",
    "#    role_assignment_ended_as_iso_date: Optional[str]\n",
    "\n",
    "class DocumentContent(BaseModel):\n",
    "    data_points: list[DocumentDataPoint]\n",
    "#    company_role_assignments: list[CompanyRoleAssignment]"
   ],
   "id": "3754c275b4be0d9b",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T07:20:50.955856Z",
     "start_time": "2024-09-03T07:20:50.950465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_document_content(text):\n",
    "    system_prompt = (\"You are an assistant with the task of extracting precise information from long documents. \"\n",
    "                     \"You will be prompted with the contents of a document. Your task is to extract various metrics \"\n",
    "#                     \"as well as company role assignments \"\n",
    "                     \"from this document. With each metric, supply the point in \"\n",
    "                     \"time when the metric was measured according to the document,\"\n",
    "                     \"as well as the currency (if applicable). \"\n",
    "                     \"If the metric is an amount, extract the exact amount (e.g. \"\n",
    "                     \"if the amount in the document is given as '100 (in thousands)' \"\n",
    "                     \"or '100k', extract the value '100000').\"\n",
    "#                     \"With each role assignment, supply when the role assignment started and ended, if possible.\"\n",
    "                     \"\\n\\n\"\n",
    "                     \"Do your best to include as many metrics for as many points in time as possible!\")                     \n",
    "    \n",
    "    from openai import OpenAI\n",
    "    client = OpenAI()\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "      ]\n",
    "    \n",
    "    prompt_info(messages)\n",
    "    response = client.beta.chat.completions.parse(\n",
    "      model=OPENAI_MODEL,\n",
    "      messages=messages,\n",
    "      response_format=DocumentContent\n",
    "    )\n",
    "    \n",
    "    data_points = [\n",
    "        {\n",
    "            \"metric_type\": x.metric_type.value,\n",
    "            \"value\": x.value,\n",
    "            \"currency\": x.currency.value if x.currency else None,\n",
    "            \"point_in_time\": x.point_in_time_as_iso_date\n",
    "        }\n",
    "        for x in response.choices[0].message.parsed.data_points\n",
    "    ]\n",
    "    \n",
    "#    role_assignments = [\n",
    "#        {\n",
    "#            \"role_type\": x.role_type.value,\n",
    "#            \"person_name\": x.person_name,\n",
    "#            \"role_assignment_started_as_iso_date\": x.role_assignment_started_as_iso_date,\n",
    "#            \"role_assignment_ended_as_iso_date\": x.role_assignment_ended_as_iso_date\n",
    "#        }\n",
    "#        for x in response.choices[0].message.parsed.company_role_assignments\n",
    "#    ]\n",
    "    \n",
    "    formatted = {\n",
    "        \"data_points\": data_points,\n",
    "#        \"role_assignments\": role_assignments\n",
    "    }\n",
    "    return formatted"
   ],
   "id": "6a634b01c9d7840a",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T07:22:33.897445Z",
     "start_time": "2024-09-03T07:20:51.005081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "\n",
    "\n",
    "# Define the paths\n",
    "samples_dir = 'samples'\n",
    "output_dir = 'output'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read the CSV file\n",
    "with open('dataset.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "    for row in csv_reader:\n",
    "        name = row['sha1'].strip().replace(',', '').replace('\"', '')  # Clean up the name to be used in filenames\n",
    "        pdf_path = os.path.join(samples_dir, f'{name}.pdf')\n",
    "        \n",
    "        # Check if the PDF file exists\n",
    "        if os.path.exists(pdf_path):\n",
    "            # Define the output path for the JSON file\n",
    "            output_path = os.path.join(output_dir, f'{name}.json')\n",
    "\n",
    "            if os.path.exists(output_path):\n",
    "                print(f'{output_path} already exists; skipping.')\n",
    "            else:\n",
    "                print(f'Processing {pdf_path}...')\n",
    "    \n",
    "                try:    \n",
    "                    # Extract text from the PDF\n",
    "                    pdf_texts = text_from_pdf(pdf_path)\n",
    "        \n",
    "                    # Extract structured content from the text\n",
    "                    structured_datas = [extract_document_content(pdf_text) for pdf_text in pdf_texts]\n",
    "                    \n",
    "                    structured_data = {\n",
    "                        \"company_name\": row['name'],\n",
    "                        \"data_points\": [item for d in structured_datas for item in d[\"data_points\"]],\n",
    "#                        \"role_assignments\": [item for d in structured_datas for item in d[\"role_assignments\"]]\n",
    "                    }\n",
    "\n",
    "                    # Save the structured data as JSON\n",
    "                    with open(output_path, 'w') as json_file:\n",
    "                        json.dump(structured_data, json_file, indent=4)\n",
    "        \n",
    "                    print(f'Saved structured data to {output_path}.')\n",
    "                except Exception as error:\n",
    "                    print(error)\n",
    "                    print(\"Exception caught; skipping...\")\n",
    "                # Uncomment this to only work with the first PDF.\n",
    "                # break\n",
    "        else:\n",
    "            # The file was not found. We ignore this, since we are only working\n",
    "            # with a small sample.\n",
    "            pass"
   ],
   "id": "36c534d070c07b64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/ac9aa244462c80705c3ff046542c02c459989742.json already exists; skipping.\n",
      "output/e2b19d2cc2ccab2fd9022326b56b38fb0e772e73.json already exists; skipping.\n",
      "output/e62b2ebe3012cd7e6c57507bc950a46d06b3d06e.json already exists; skipping.\n",
      "output/e765cdd472cb47fa74ee6a52700c61aca645bbee.json already exists; skipping.\n",
      "output/6054ec55767fbe6585598ced7afacf5cb8619a13.json already exists; skipping.\n",
      "output/9d7a72445aba6860402c3acce75af02dc045f74d.json already exists; skipping.\n",
      "output/a706b44ba275c97b8633b0808cd2f90cbb7fe473.json already exists; skipping.\n",
      "output/a8077fe1983a64dc77bddfafbf48242e66111a89.json already exists; skipping.\n",
      "output/84749ef5c2bbf2a302b6614f31727a95bf29f309.json already exists; skipping.\n",
      "output/ba5852cb6c20da35da2ce6ebaafc711d06fe8c1e.json already exists; skipping.\n",
      "Processing samples/609042c64a759c0ac63e7cf18742be4dd3cc5cd5.pdf...\n",
      "splitting\n",
      "Tokens: 92850\n",
      "Cost: 0.46425â‚¬\n",
      "Tokens: 99322\n",
      "Cost: 0.49661â‚¬\n",
      "Saved structured data to output/609042c64a759c0ac63e7cf18742be4dd3cc5cd5.json.\n",
      "Processing samples/e0d6bb578bfc233c3bdbded229960fed01624631.pdf...\n",
      "splitting\n",
      "Tokens: 81343\n",
      "Cost: 0.406715â‚¬\n",
      "Tokens: 87989\n",
      "Cost: 0.439945â‚¬\n",
      "Saved structured data to output/e0d6bb578bfc233c3bdbded229960fed01624631.json.\n",
      "output/43437bccca01aeae222ec7cc706ba3eaf50be2d7.json already exists; skipping.\n",
      "output/194000c9109c6fa628f1fed33b44ae4c2b8365f4.json already exists; skipping.\n",
      "output/f06d7ecc8072de616a4ea35c74e20199de6b0691.json already exists; skipping.\n",
      "output/85fb23ba2910de45e27f8f40170c0f3576043916.json already exists; skipping.\n",
      "output/2779336b845a41544348abb7b3e6e5bd2ff893a2.json already exists; skipping.\n",
      "output/f721fa86aa0f17b194fa21917aeabbf8df6511c2.json already exists; skipping.\n",
      "output/cbd8fb252e8743dc53961adc53e2b4de9ff6037a.json already exists; skipping.\n",
      "output/e33544bdea57faa0ad10ba2e93bf052482f33325.json already exists; skipping.\n"
     ]
    }
   ],
   "execution_count": 96
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
