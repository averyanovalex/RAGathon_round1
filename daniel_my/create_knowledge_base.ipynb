{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T08:17:12.377668Z",
     "start_time": "2024-09-04T08:17:12.363156Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73aa1ff879683fbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T08:17:12.443356Z",
     "start_time": "2024-09-04T08:17:12.413640Z"
    }
   },
   "outputs": [],
   "source": [
    "# import PyPDF2\n",
    "\n",
    "# def split_rec(full_text):\n",
    "#     # Initialize tiktoken encoding for the specified model\n",
    "#     encoding = tiktoken.encoding_for_model(OPENAI_MODEL)    \n",
    "#     # Count the number of tokens in the extracted text\n",
    "#     num_tokens = len(encoding.encode(full_text))\n",
    "    \n",
    "#     # Check if the number of tokens exceeds the limit (128K)\n",
    "#     # or the number of characters \n",
    "#     if num_tokens > 100000 or len(full_text) > 900000:\n",
    "#         print(\"splitting\")\n",
    "#         # Split the pages into two roughly equal parts\n",
    "#         mid_point = len(full_text) // 2\n",
    "#         part1 = full_text[:mid_point]\n",
    "#         part2 = full_text[mid_point:]\n",
    "        \n",
    "#         split1 = split_rec(part1)\n",
    "#         split2 = split_rec(part2)\n",
    "        \n",
    "#         return split1 + split2\n",
    "#     else:\n",
    "#         # Return the full text as a single-element list\n",
    "#         return [full_text]    \n",
    "\n",
    "# def text_from_pdf(file_path):\n",
    "#     # Initialize an empty list to store the text of each page\n",
    "#     pages_text = []\n",
    "\n",
    "#     # Read the PDF file\n",
    "#     with open(file_path, 'rb') as f:\n",
    "#         reader = PyPDF2.PdfReader(f, strict=False)\n",
    "#         for page in reader.pages:\n",
    "#             pages_text.append(page.extract_text())\n",
    "    \n",
    "#     # Combine all the text to check the total token count\n",
    "#     full_text = \"\\n\".join(pages_text)\n",
    "\n",
    "#     return split_rec(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da8cea1c74a874e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T08:17:12.549774Z",
     "start_time": "2024-09-04T08:17:12.547370Z"
    }
   },
   "outputs": [],
   "source": [
    "# OPENAI_MODEL=\"gpt-4o-2024-08-06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7a2ffc328682dd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T08:17:13.119228Z",
     "start_time": "2024-09-04T08:17:12.599381Z"
    }
   },
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "\n",
    "# def merge_usage_report_items(items):\n",
    "#     return {\n",
    "#         \"my_tokens\": sum([i[\"my_tokens\"] for i in items]),\n",
    "#         \"prompt_tokens\": sum([i[\"prompt_tokens\"] for i in items]),\n",
    "#         \"completion_tokens\": sum([i[\"completion_tokens\"] for i in items]),\n",
    "#         \"total_tokens\": sum([i[\"total_tokens\"] for i in items]),\n",
    "#         \"total_cost_usd\": sum([i[\"total_cost_usd\"] for i in items]),\n",
    "#     }\n",
    "\n",
    "# enc = tiktoken.encoding_for_model(OPENAI_MODEL)\n",
    "# input_token_cost_usd_per_1m_tokens = 2.5\n",
    "# output_token_cost_usd_per_1m_tokens = 10\n",
    "# _1m = 1000000\n",
    "\n",
    "# def get_usage_report(messages, response):\n",
    "#     content = \" \".join([m[\"content\"] for m in messages])\n",
    "#     my_tokens = len(enc.encode(content))\n",
    "#     print(f\"My Tokens: {my_tokens}\")\n",
    "\n",
    "#     prompt_tokens = response.usage.prompt_tokens\n",
    "#     print(f\"Prompt Tokens: {prompt_tokens}\")\n",
    "\n",
    "#     completion_tokens = response.usage.completion_tokens\n",
    "#     print(f\"Completion Tokens: {completion_tokens}\")\n",
    "\n",
    "#     prompt_cost_in_usd = (prompt_tokens / _1m) * input_token_cost_usd_per_1m_tokens\n",
    "#     completion_cost_in_usd = (completion_tokens / _1m) * output_token_cost_usd_per_1m_tokens\n",
    "#     total_cost_usd = prompt_cost_in_usd + completion_cost_in_usd\n",
    "#     print(f\"Cost: ${total_cost_usd}\")\n",
    "    \n",
    "#     total_tokens = response.usage.total_tokens\n",
    "    \n",
    "#     if total_tokens != prompt_tokens + completion_tokens:\n",
    "#         print(\"WARN: token counts don't match\")\n",
    "#         print(total_tokens)\n",
    "#         print(prompt_tokens)\n",
    "#         print(completion_tokens)\n",
    "#         print(prompt_tokens+completion_tokens)\n",
    "    \n",
    "#     return {\n",
    "#         \"my_tokens\": my_tokens,\n",
    "#         \"prompt_tokens\": prompt_tokens,\n",
    "#         \"completion_tokens\": completion_tokens,\n",
    "#         \"total_tokens\": total_tokens,\n",
    "#         \"total_cost_usd\": total_cost_usd,        \n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3754c275b4be0d9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T08:17:13.213024Z",
     "start_time": "2024-09-04T08:17:13.126093Z"
    }
   },
   "outputs": [],
   "source": [
    "# from pydantic import BaseModel\n",
    "# from typing import Optional\n",
    "# from enum import Enum\n",
    "\n",
    "# class Metric(str, Enum):\n",
    "#     rad_expenses = \"research and development expenses\"\n",
    "#     risk_management_spending = \"risk management spending\"\n",
    "# #    debt_to_equity_ratio = \"Debt-To-Equity ratio\"\n",
    "# #    number_of_stores = \"Number of stores\"\n",
    "#     return_on_assets = \"Return on Assets (ROA)\"\n",
    "# #    return_on_equity = \"Return on Equity (ROE)\"\n",
    "#     customer_acquisition_spending = \"customer acquisition spending\"\n",
    "#     operating_margin = \"operating margin\"\n",
    "#     market_capitalization = \"market capitalization\"\n",
    "#     sustainability_initiatives_spending = \"sustainability initiatives spending\"\n",
    "#     gross_profit_margin = \"gross profit margin\"\n",
    "#     net_profit_margin = \"net profit margin\"\n",
    "#     total_liabilities = \"total liabilities\"\n",
    "#     total_assets = \"total assets\"\n",
    "#     intangible_assets = \"intangible assets\"\n",
    "#     marketing_spending = \"marketing spending\"\n",
    "#     free_cash_flow = \"free cash flow\"\n",
    "#     earnings_per_share = \"earnings per share (EPS)\"\n",
    "#     accounts_receivable = \"accounts_receivable\"\n",
    "#     acquisition_costs = \"acquisition costs\"\n",
    "#     shareholders_equity = \"shareholders' equity\"\n",
    "#     operating_cash_flow = \"operating cash flow\"\n",
    "#     quick_ratio = \"Quick Ratio\"\n",
    "#     net_income = \"net income\"\n",
    "#     inventory = \"inventory\"\n",
    "#     total_revenue = \"total revenue\"\n",
    "\n",
    "# #class CompanyRole(str, Enum):\n",
    "# #    ceo = \"Chief Executive Officer (CEO)\"\n",
    "# #    cfo = \"Chief Financial Officer (CFO)\"\n",
    "# #    coo = \"Chief Operating Officer (COO)\"\n",
    "# #    clo = \"Chief Legal Officer (CLO)\"\n",
    "# #    board_chairman = \"Board Chairman\"\n",
    "\n",
    "# class Currency(str, Enum):\n",
    "#     euro = \"EUR\"\n",
    "#     us_dollar = \"USD\"\n",
    "#     great_britain_pound = \"GBP\"\n",
    "#     australian_dollar = \"AUD\"\n",
    "#     other = \"OTHER\"\n",
    "\n",
    "# class DocumentDataPoint(BaseModel):\n",
    "#     metric_type: Metric\n",
    "#     value: float\n",
    "#     currency: Optional[Currency]\n",
    "#     point_in_time_as_iso_date: str\n",
    "\n",
    "# #class CompanyRoleAssignment(BaseModel):\n",
    "# #    role_type: CompanyRole\n",
    "# #    person_name: str\n",
    "# #    role_assignment_started_as_iso_date: Optional[str]\n",
    "# #    role_assignment_ended_as_iso_date: Optional[str]\n",
    "\n",
    "# class DocumentContent(BaseModel):\n",
    "#     data_points: list[DocumentDataPoint]\n",
    "# #    company_role_assignments: list[CompanyRoleAssignment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a634b01c9d7840a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T08:17:13.224480Z",
     "start_time": "2024-09-04T08:17:13.219687Z"
    }
   },
   "outputs": [],
   "source": [
    "# def extract_document_content(text):\n",
    "#     system_prompt = (\"You are an assistant with the task of extracting precise information from long documents. \"\n",
    "#                      \"You will be prompted with the contents of a document. Your task is to extract various metrics \"\n",
    "# #                     \"as well as company role assignments \"\n",
    "#                      \"from this document. With each metric, supply the point in \"\n",
    "#                      \"time when the metric was measured according to the document,\"\n",
    "#                      \"as well as the currency (if applicable). \"\n",
    "#                      \"If the metric is an amount, extract the exact amount (e.g. \"\n",
    "#                      \"if the amount in the document is given as '100 (in thousands)' \"\n",
    "#                      \"or '100k', extract the value '100000').\"\n",
    "# #                     \"With each role assignment, supply when the role assignment started and ended, if possible.\"\n",
    "#                      \"\\n\\n\"\n",
    "#                      \"Do your best to include as many metrics for as many points in time as possible!\")                     \n",
    "    \n",
    "#     from openai import OpenAI\n",
    "#     client = OpenAI()\n",
    "    \n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": system_prompt},\n",
    "#         {\"role\": \"user\", \"content\": text},\n",
    "#       ]\n",
    "    \n",
    "    \n",
    "#     response = client.beta.chat.completions.parse(\n",
    "#       model=OPENAI_MODEL,\n",
    "#       messages=messages,\n",
    "#       response_format=DocumentContent\n",
    "#     )\n",
    "    \n",
    "#     usage_report = get_usage_report(messages, response)\n",
    "    \n",
    "#     data_points = [\n",
    "#         {\n",
    "#             \"metric_type\": x.metric_type.value,\n",
    "#             \"value\": x.value,\n",
    "#             \"currency\": x.currency.value if x.currency else None,\n",
    "#             \"point_in_time\": x.point_in_time_as_iso_date\n",
    "#         }\n",
    "#         for x in response.choices[0].message.parsed.data_points\n",
    "#     ]\n",
    "    \n",
    "# #    role_assignments = [\n",
    "# #        {\n",
    "# #            \"role_type\": x.role_type.value,\n",
    "# #            \"person_name\": x.person_name,\n",
    "# #            \"role_assignment_started_as_iso_date\": x.role_assignment_started_as_iso_date,\n",
    "# #            \"role_assignment_ended_as_iso_date\": x.role_assignment_ended_as_iso_date\n",
    "# #        }\n",
    "# #        for x in response.choices[0].message.parsed.company_role_assignments\n",
    "# #    ]\n",
    "    \n",
    "#     result = {\n",
    "#         \"data_points\": data_points,\n",
    "# #        \"role_assignments\": role_assignments\n",
    "#     }\n",
    "#     return result, usage_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36c534d070c07b64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T08:27:17.744256Z",
     "start_time": "2024-09-04T08:17:13.264016Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "\n",
    "\n",
    "# Define the paths\n",
    "samples_dir = '../samples'\n",
    "output_dir = 'output'\n",
    "\n",
    "usage_reports = {}\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read the CSV file\n",
    "with open('dataset.csv', 'r') as csv_file:\n",
    "#     csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "#     for row in csv_reader:\n",
    "#         name = row['sha1'].strip().replace(',', '').replace('\"', '')  # Clean up the name to be used in filenames\n",
    "#         pdf_path = os.path.join(samples_dir, f'{name}.pdf')\n",
    "        \n",
    "#         # Check if the PDF file exists\n",
    "#         if os.path.exists(pdf_path):\n",
    "#             # Define the output path for the JSON file\n",
    "#             output_path = os.path.join(output_dir, f'{name}.json')\n",
    "\n",
    "#             if os.path.exists(output_path):\n",
    "#                 print(f'{output_path} already exists; skipping.')\n",
    "#             else:\n",
    "#                 print(f'Processing {pdf_path}...')\n",
    "    \n",
    "#                 try:    \n",
    "#                     # Extract text from the PDF\n",
    "#                     pdf_texts = text_from_pdf(pdf_path)\n",
    "        \n",
    "#                     # Extract structured content from the text\n",
    "#                     # Involves \"unzipping\" following https://stackoverflow.com/questions/12974474/how-to-unzip-a-list-of-tuples-into-individual-lists.\n",
    "#                     [structured_datas, usage_report_items] = [list(t) for t in zip(*[extract_document_content(pdf_text) for pdf_text in pdf_texts])]\n",
    "                    \n",
    "#                     usage_reports[name] = merge_usage_report_items(usage_report_items)\n",
    "                    \n",
    "#                     structured_data = {\n",
    "#                         \"company_name\": row['name'],\n",
    "#                         \"data_points\": [item for d in structured_datas for item in d[\"data_points\"]],\n",
    "# #                        \"role_assignments\": [item for d in structured_datas for item in d[\"role_assignments\"]]\n",
    "#                     }\n",
    "\n",
    "#                     # Save the structured data as JSON\n",
    "#                     with open(output_path, 'w') as json_file:\n",
    "#                         json.dump(structured_data, json_file, indent=4)\n",
    "        \n",
    "#                     print(f'Saved structured data to {output_path}.')\n",
    "#                 except Exception as error:\n",
    "#                     print(error)\n",
    "#                     print(\"Exception caught; skipping...\")\n",
    "#                 # Uncomment this to only work with the first PDF.\n",
    "#                 # break\n",
    "#         else:\n",
    "#             # The file was not found. We ignore this, since we are only working\n",
    "#             # with a small sample.\n",
    "#             pass\n",
    "\n",
    "# usage_report = {\n",
    "#     \"summary\": merge_usage_report_items(usage_reports.values()),\n",
    "#     \"details\": usage_reports\n",
    "# }\n",
    "\n",
    "# with open('usage_report_create_knowledge_base.json', 'w') as json_file:\n",
    "#     json.dump(usage_report, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153e9f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
